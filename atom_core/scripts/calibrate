#!/usr/bin/env python
"""
Reads a set of data and labels from a group of sensors in a json file and calibrates the poses of these sensors.
"""

# -------------------------------------------------------------------------------
# --- IMPORTS (standard, then third party, then own modules)
# -------------------------------------------------------------------------------
import os
from copy import deepcopy
import numpy as np
from functools import partial
import json
import sys
import argparse
from colorama import Fore, Style
import cv2
import signal
import sys
import tf

import OptimizationUtils.OptimizationUtils as OptimizationUtils
import OptimizationUtils.utilities as optimization_utilities
import atom_calibration.utilities as utilities
import atom_core.getters_and_setters as getters_and_setters
import atom_core.objective_function as objective_function
import atom_core.patterns as patterns
import atom_core.visualization as visualization


# -------------------------------------------------------------------------------
# --- FUNCTIONS
# -------------------------------------------------------------------------------
def signal_handler(sig, frame):
    print('Stopping optimization (Ctrl+C pressed)')
    sys.exit(0)


def is_jsonable(x):
    try:
        json.dumps(x)
        return True
    except (TypeError, OverflowError):
        return False


def walk(node):
    for key, item in node.items():
        if isinstance(item, dict):
            walk(item)
        else:
            if isinstance(item, np.ndarray) and key == 'data':  # to avoid saving images in the json
                del node[key]

            elif isinstance(item, np.ndarray):
                node[key] = item.tolist()
            pass


def fitPlaneLTSQ(XYZ):
    (rows, cols) = XYZ.shape
    G = np.ones((rows, 3))
    G[:, 0] = XYZ[:, 0]  # X
    G[:, 1] = XYZ[:, 1]  # Y
    Z = XYZ[:, 2]
    (a, b, c), resid, rank, s = np.linalg.lstsq(G, Z)
    normal = (a, b, -1)
    nn = np.linalg.norm(normal)
    normal = normal / nn
    return (c, normal)


# Save to json file
def createJSONFile(output_file, input):
    D = deepcopy(input)

    walk(D)

    print("Saving the json output file to " + str(output_file) + ", please wait, it could take a while ...")
    f = open(output_file, 'w')
    json.encoder.FLOAT_REPR = lambda f: ("%.6f" % f)  # to get only four decimal places on the json file
    print >> f, json.dumps(D, indent=2, sort_keys=True)
    f.close()
    print("Completed.")


# -------------------------------------------------------------------------------
# --- MAIN
# -------------------------------------------------------------------------------
def main():
    # ---------------------------------------
    # --- Parse command line argument
    # ---------------------------------------
    signal.signal(signal.SIGINT, signal_handler)
    # print('Press Ctrl+C')
    # signal.pause()

    ap = argparse.ArgumentParser()
    ap = OptimizationUtils.addArguments(ap)  # OptimizationUtils arguments
    ap.add_argument("-json", "--json_file", help="Json file containing input dataset.", type=str, required=True)
    ap.add_argument("-rv", "--ros_visualization", help="Publish ros visualization markers.", action='store_true')
    ap.add_argument("-si", "--show_images", help="shows images for each camera", action='store_true', default=False)
    ap.add_argument("-uic", "--use_incomplete_collections",
                    help="Remove any collection which does not have a detection for all sensors.",
                    action='store_true', default=False)
    ap.add_argument("-rpd", "--remove_partial_detections",
                    help="Remove detected labels which are only partial. Used or the Charuco.",
                    action='store_true', default=False)
    ap.add_argument("-ssf", "--sensor_selection_function", default=None, type=utilities.create_lambda_with_globals,
                    help='A string to be evaluated into a lambda function that receives a sensor name as input and '
                         'returns True or False to indicate if the sensor should be loaded (and used in the '
                         'optimization). The Syntax is lambda name: f(x), where f(x) is the function in python '
                         'language. Example: lambda name: name in ["left_laser", "frontal_camera"] , to load only '
                         'sensors left_laser and frontal_camera')
    ap.add_argument("-csf", "--collection_selection_function", default=None, type=utilities.create_lambda_with_globals,
                    help='A string to be evaluated into a lambda function that receives a collection name as input and '
                         'returns True or False to indicate if the collection should be loaded (and used in the '
                         'optimization). The Syntax is lambda name: f(x), where f(x) is the function in python '
                         'language. Example: lambda name: int(name) > 5 , to load only collections 6, 7, and onward.')

    args = vars(ap.parse_args(args=utilities.filterLaunchArguments(sys.argv)))
    print("\nArgument list=" + str(args) + '\n')

    # ---------------------------------------
    # --- Reading robot description file from param /robot_description
    # ---------------------------------------
    # rospy.loginfo('Reading /robot_description ros param')
    # xml_robot = URDF.from_parameter_server()  # needed to create the optimized xacro at the end of the optimization

    # ---------------------------------------
    # --- INITIALIZATION Read data from file
    # ---------------------------------------
    """ Loads a json file containing the detections"""
    json_file, _, _ = utilities.uriReader(args['json_file'])
    f = open(json_file, 'r')
    dataset = json.load(f)

    # Load images from files into memory. Images in the json file are stored in separate png files and in their place
    # a field "data_file" is saved with the path to the file. We must load the images from the disk.
    for collection_key, collection in dataset['collections'].items():
        for sensor_key, sensor in dataset['sensors'].items():
            if not sensor['msg_type'] == 'Image':  # nothing to do here.
                continue

            filename = os.path.dirname(json_file) + '/' + collection['data'][sensor_key]['data_file']
            collection['data'][sensor_key]['data'] = cv2.imread(filename)

    if not args['collection_selection_function'] is None:
        deleted = []
        for collection_key in dataset['collections'].keys():
            if not args['collection_selection_function'](collection_key):  # use the lambda expression csf
                deleted.append(collection_key)
                del dataset['collections'][collection_key]
        print("Deleted collections: " + str(deleted))

    if not args['use_incomplete_collections']:
        # Deleting collections where the pattern is not found by all sensors:
        for collection_key, collection in dataset['collections'].items():
            for sensor_key, sensor in dataset['sensors'].items():
                if not collection['labels'][sensor_key]['detected']:
                    print(Fore.RED + "Removing collection " + collection_key + ' -> pattern was not found in sensor ' +
                          sensor_key + ' (must be found in all sensors).' + Style.RESET_ALL)
                    del dataset['collections'][collection_key]
                    break

    if args['remove_partial_detections']:
        number_of_corners = int(dataset['calibration_config']['calibration_pattern']['dimension']['x']) * \
                            int(dataset['calibration_config']['calibration_pattern']['dimension']['y'])
        # Deleting labels in which not all corners are found:
        for collection_key, collection in dataset['collections'].items():
            for sensor_key, sensor in dataset['sensors'].items():
                if sensor['msg_type'] == 'Image' and collection['labels'][sensor_key]['detected']:
                    if not len(collection['labels'][sensor_key]['idxs']) == number_of_corners:
                        print(Fore.RED + 'Partial detection removed:' + Style.RESET_ALL + ' label from collection ' +
                              collection_key + ', sensor ' + sensor_key)
                        collection['labels'][sensor_key]['detected'] = False

    print("\nCollections studied:\n " + str(dataset['collections'].keys()))

    # ---------------------------------------
    # --- CREATE CHESSBOARD DATASET
    # ---------------------------------------
    dataset['patterns'] = patterns.createPatternLabels(args, dataset)

    # ---------------------------------------
    # --- FILTER SOME OF THE ELEMENTS LOADED, TO USE ONLY A SUBSET IN THE CALIBRATION
    # ---------------------------------------
    if not args['sensor_selection_function'] is None:
        deleted = []
        print(args['sensor_selection_function'])
        for sensor_key in dataset['sensors'].keys():
            if not args['sensor_selection_function'](sensor_key):  # use the lambda expression ssf
                deleted.append(sensor_key)
                del dataset['sensors'][sensor_key]
        print("Deleted sensors: " + str(deleted))

    print('Loaded dataset containing ' + str(len(dataset['sensors'].keys())) + ' sensors and ' + str(
        len(dataset['collections'].keys())) + ' collections.')

    # ---------------------------------------
    # --- DETECT EDGES IN THE LASER SCANS
    # ---------------------------------------
    for sensor_key, sensor in dataset['sensors'].items():
        if sensor['msg_type'] == 'LaserScan':  # only for lasers
            for collection_key, collection in dataset['collections'].items():
                idxs = collection['labels'][sensor_key]['idxs']
                edges = []  # a list of edges
                for i in range(0, len(idxs) - 1):
                    if (idxs[i + 1] - idxs[i]) != 1:
                        edges.append(i)
                        edges.append(i + 1)

                # Remove first (right most) and last (left most) edges, since these are often false edges.
                if len(edges) > 0:
                    edges.pop(0)  # remove the first element.
                if len(edges) > 0:
                    edges.pop()  # if the index is not given, then the last element is popped out and removed.
                collection['labels'][sensor_key]['edge_idxs'] = edges

    # ---------------------------------------
    # --- Detect corners in velodyne data
    # ---------------------------------------
    for sensor_key, sensor in dataset['sensors'].items():
        if sensor['msg_type'] == 'PointCloud2':  # only for 3D Lidars and RGBD cameras
            for collection_key, collection in dataset['collections'].items():
                import ros_numpy
                from scipy.spatial import distance
                from rospy_message_converter import message_converter

                # Convert 3D cloud data on .json dictionary to ROS message type
                cloud_msg = message_converter.convert_dictionary_to_ros_message("sensor_msgs/PointCloud2",
                                                                                collection['data'][sensor_key])

                # ------------------------------------------------------------------------------------------------
                # -------- Extract the labelled LiDAR points on the pattern
                # ------------------------------------------------------------------------------------------------
                idxs = collection['labels'][sensor_key]['idxs']
                pc = ros_numpy.numpify(cloud_msg)[idxs]
                points = np.zeros((pc.shape[0], 4))
                points[:, 0] = pc['x']
                points[:, 1] = pc['y']
                points[:, 2] = pc['z']
                points[:, 3] = 1
                collection['labels'][sensor_key]['labelled_points'] = points

                # ------------------------------------------------------------------------------------------------
                # -------- Convert cloud labelled points to 2D pattern frame
                # ------------------------------------------------------------------------------------------------
                c, normal = fitPlaneLTSQ(points[:, 0:3])  # Fit plane to points
                plane_a, plane_b, plane_c = normal[0], normal[1], normal[2]
                point = np.array([0.0, 0.0, c])
                plane_d = -point.dot(normal)

                # Define a point in the plane which is the first of the list, projected to the plane
                # use average y and z values (since velodyne uses x forward) to put origin on the center of the pattern
                pt_avg = np.average(points[:, 0:3], axis=0)

                y, z = pt_avg[1], pt_avg[2]
                x = (- plane_b * y - plane_c * z - plane_d) / plane_a
                p0 = np.array([x, y, z])

                # Define a second point. We use the first point from the list of points, and project it to the plane
                idx = 0
                y, z = points[idx, 1], points[idx, 2]
                x = (- plane_b * y - plane_c * z - plane_d) / plane_a
                p1 = np.array([x, y, z])

                # Define a rotation matrix, and the n (direction vector along the x axis), s and a vectors:
                #        n    s    a
                # R = [ r11  r12  r13 ]
                #     [ r21  r22  r23 ]
                #     [ r31  r32  r33 ]
                n = p1 - p0  # semi-arbitrary x-axis: only constraint is that it is coplanar with the plane. This is
                # ensured since both p0 and p1 lie on the plane
                a = normal  # z axis must have the direction normal to the plane
                s = np.cross(a, n)  # y axis is the cross product of z axis per the x axis

                def getUnitVector(v):
                    return v / np.linalg.norm(v)

                n, s, a = getUnitVector(n), getUnitVector(s), getUnitVector(a)
                # - Compute homogeneous transformation from velodyne to local reference system
                T = np.zeros((4, 4), np.float32)
                R = np.zeros((3, 3), np.float32)
                R[:, 0] = n
                R[:, 1] = s
                R[:, 2] = a
                trans = p0  # we use on of the points in the plane to define the translation
                T[0:3, 0:3] = R
                T[0:3, 3] = trans
                T[3, 3] = 1
                # - Convert to ros tf and save in transforms dictionary
                euler = tf.transformations.euler_from_matrix(R)
                quat = tf.transformations.quaternion_from_euler(euler[0], euler[1], euler[2])
                parent = sensor['parent']
                child = 'pattern_link_from_' + parent
                key = optimization_utilities.generateKey(parent, child)
                collection['transforms'][key] = {'child': child, 'parent': parent, 'quat': list(quat),
                                                 'trans': list(trans)}

                # - Convert velodyne points into the 2D pattern reference frame
                pts_in_pattern = np.dot(np.linalg.inv(T), points.transpose())
                pts_in_pattern = pts_in_pattern.transpose()[:, 0:2]

                # ------------------------------------------------------------------------------------------------
                # -------- Find extrema points using circle radius fitting
                # ------------------------------------------------------------------------------------------------
                # CONFIGURATIONS
                start_angle = 0.  # start angle increment in 0 radians
                end_angle = 2. * np.pi  # end the angle increment when we complete the whole circle
                angle_step = 0.05 * np.pi / 180.  # angle step in radians
                # Compute circle radius using the higher distance between two points in the array
                pts_copy_a = list(pts_in_pattern)
                pts_copy_b = list(pts_in_pattern)
                radius = np.max(distance.cdist(pts_copy_a, pts_copy_b, 'euclidean')) / 2. + 0.1
                # Compute a circle of points outside the pattern and find the pattern point closer to each source point
                extrema_points = []
                seed_point = np.average(pts_in_pattern, axis=0)
                th = start_angle
                print('Computing 3D cloud pattern limit points for collection ' + str(collection_key))
                while th < end_angle:
                    # Compute the source points out of the pattern bounds
                    source_pt = [[seed_point[0] + radius * np.cos(th),
                                  seed_point[1] + radius * np.sin(th)]]
                    # Compute point that is more distant from the seed point
                    c_idx = np.argmin(distance.cdist(source_pt, pts_in_pattern[:, :], 'euclidean'))
                    extrema_points.append(points[c_idx, :])

                    th += angle_step
                    # ----------------

                # Delete duplicates from extrema points array
                unique_extrema_points = list(set(map(tuple, extrema_points)))
                collection['labels'][sensor_key]['limit_points'] = unique_extrema_points

    # ---------------------------------------
    # --- SETUP OPTIMIZER
    # ---------------------------------------
    opt = OptimizationUtils.Optimizer()
    opt.addDataModel('args', args)
    opt.addDataModel('dataset', dataset)

    # For the getters we only need to get one collection. Lets take the first key on the dictionary and always get that
    # transformation.
    selected_collection_key = dataset['collections'].keys()[0]

    # ------------  Sensors -----------------
    # Each sensor will have a position (tx,ty,tz) and a rotation (r1,r2,r3)

    # Add parameters related to the sensors
    translation_delta = 0.01

    # TODO temporary placement of top_left_camera
    # for collection_key, collection in dataset['collections'].items():
    #     collection['transforms']['base_link-top_left_camera']['trans'] = [-1.48, 0.22, 1.35]
    # dataset['calibration_config']['anchored_sensor'] = 'right_laser'
    print('Anchored sensor is ' + Fore.GREEN + dataset['calibration_config'][
        'anchored_sensor'] + Style.RESET_ALL)

    print('Creating parameters ...')
    for sensor_key, sensor in dataset['sensors'].items():

        # Translation -------------------------------------
        initial_translation = getters_and_setters.getterSensorTranslation(dataset, sensor_key=sensor_key,
                                                                          collection_key=selected_collection_key)

        if sensor_key == dataset['calibration_config']['anchored_sensor']:
            bound_max = [x + sys.float_info.epsilon for x in initial_translation]
            bound_min = [x - sys.float_info.epsilon for x in initial_translation]
        else:
            # bound_max = [x + translation_delta for x in initial_translation]
            # bound_min = [x - translation_delta for x in initial_translation]
            bound_max = [+np.inf for x in initial_translation]
            bound_min = [-np.inf for x in initial_translation]
            if 'laser' in sensor_key:
                bound_max[2] = initial_translation[2] + translation_delta
                bound_min[2] = initial_translation[2] - translation_delta

        opt.pushParamVector(group_name='S_' + sensor_key + '_t', data_key='dataset',
                            getter=partial(getters_and_setters.getterSensorTranslation, sensor_key=sensor_key,
                                           collection_key=selected_collection_key),
                            setter=partial(getters_and_setters.setterSensorTranslation, sensor_key=sensor_key),
                            suffix=['x', 'y', 'z'],
                            bound_max=bound_max, bound_min=bound_min)

        # Rotation --------------------------------------
        initial_rotation = getters_and_setters.getterSensorRotation(dataset, sensor_key=sensor_key,
                                                                    collection_key=selected_collection_key)

        if sensor_key == dataset['calibration_config']['anchored_sensor']:
            bound_max = [x + sys.float_info.epsilon for x in initial_rotation]
            bound_min = [x - sys.float_info.epsilon for x in initial_rotation]
        else:
            bound_max = [+np.inf for x in initial_rotation]
            bound_min = [-np.inf for x in initial_rotation]

        opt.pushParamVector(group_name='S_' + sensor_key + '_r', data_key='dataset',
                            getter=partial(getters_and_setters.getterSensorRotation, sensor_key=sensor_key,
                                           collection_key=selected_collection_key),
                            setter=partial(getters_and_setters.setterSensorRotation, sensor_key=sensor_key),
                            suffix=['1', '2', '3'],
                            bound_max=bound_max, bound_min=bound_min)

        if sensor['msg_type'] == 'Image':  # if sensor is a camera add intrinsics
            # opt.pushParamVector(group_name='S_' + sensor_key + '_I_', data_key='dataset',
            #                     getter=partial(getterCameraIntrinsics, sensor_key=sensor_key),
            #                     setter=partial(setterCameraIntrinsics, sensor_key=sensor_key),
            #                     suffix=['fx', 'fy', 'cx', 'cy', 'd0', 'd1', 'd2', 'd3', 'd4'])
            opt.pushParamVector(group_name='S_' + sensor_key + '_P_', data_key='dataset',
                                getter=partial(getters_and_setters.getterCameraPMatrix, sensor_key=sensor_key),
                                setter=partial(getters_and_setters.setterCameraPMatrix, sensor_key=sensor_key),
                                suffix=['fx_p', 'fy_p', 'cx_p', 'cy_p'])

    # ------------  Patterns -----------------
    # Each Pattern will have the position (tx,ty,tz) and rotation (r1,r2,r3)

    # Add translation and rotation parameters related to the Chessboards
    for collection_key in dataset['patterns']['collections']:
        # bound_max = [x + translation_delta for x in initial_values]
        # bound_min = [x - translation_delta for x in initial_values]

        # initial_translation = getterChessBoardTranslation(dataset,collection_key)
        # initial_translation[0] += 0.9
        # initial_translation[1] += 0.7
        # initial_translation[2] += 0.7
        # setterChessBoardTranslation(dataset, initial_translation, collection_key)

        opt.pushParamVector(group_name='C_' + collection_key + '_t', data_key='dataset',
                            getter=partial(getters_and_setters.getterPatternTranslation,
                                           collection_key=collection_key),
                            setter=partial(getters_and_setters.setterPatternTranslation,
                                           collection_key=collection_key),
                            suffix=['x', 'y', 'z'])
        # ,bound_max=bound_max, bound_min=bound_min)

        opt.pushParamVector(group_name='C_' + collection_key + '_r', data_key='dataset',
                            getter=partial(getters_and_setters.getterPatternRotation, collection_key=collection_key),
                            setter=partial(getters_and_setters.setterPatternRotation, collection_key=collection_key),
                            suffix=['1', '2', '3'])

    # ---------------------------------------
    # --- Define THE OBJECTIVE FUNCTION
    # ---------------------------------------
    opt.setObjectiveFunction(objective_function.objectiveFunction)

    # ---------------------------------------
    # --- Define THE RESIDUALS
    # ---------------------------------------
    # Each error is computed after the sensor and the pattern of a collection. Thus, each error will be affected
    # by the parameters tx,ty,tz,r1,r2,r3 of the sensor and the pattern

    print("Creating residuals ... ")
    for collection_key, collection in dataset['collections'].items():
        for sensor_key, sensor in dataset['sensors'].items():
            if not collection['labels'][sensor_key]['detected']:  # if pattern not detected by sensor in collection
                continue

            params = opt.getParamsContainingPattern('S_' + sensor_key)  # sensor related params
            params.extend(opt.getParamsContainingPattern('C_' + collection_key + '_'))  # pattern related params

            if sensor['msg_type'] == 'Image':  # if sensor is a camera use four residuals
                # for idx in range(0, 4):
                # for idx in range(0, len()):
                # for idx in range(0, len(dataset['patterns']['corners'])):
                for idx in collection['labels'][sensor_key]['idxs']:
                    rname = str(collection_key) + '_' + str(sensor_key) + '_' + str(idx['id'])
                    opt.pushResidual(name=rname, params=params)

            elif sensor['msg_type'] == 'LaserScan':  # if sensor is a 2D lidar add two residuals

                # Extrema points (longitudinal error)
                opt.pushResidual(name=collection_key + '_' + sensor_key + '_eleft', params=params)
                opt.pushResidual(name=collection_key + '_' + sensor_key + '_eright', params=params)

                # Inner points, use detection of edges (longitudinal error)
                for idx, _ in enumerate(collection['labels'][sensor_key]['edge_idxs']):
                    opt.pushResidual(name=collection_key + '_' + sensor_key + '_inner_' + str(idx), params=params)

                # Laser beam (orthogonal error)
                for idx in range(0, len(collection['labels'][sensor_key]['idxs'])):
                    opt.pushResidual(name=collection_key + '_' + sensor_key + '_beam_' + str(idx), params=params)

            elif sensor['msg_type'] == 'PointCloud2':  # if sensor is a 3D lidar add two types of residuals

                # Laser beam error
                for idx in range(0, len(collection['labels'][sensor_key]['idxs'])):
                    opt.pushResidual(name=collection_key + '_' + sensor_key + '_oe_' + str(idx), params=params)
                # Extrema displacement error
                for idx in range(0, len(collection['labels'][sensor_key]['limit_points'])):
                    opt.pushResidual(name=collection_key + '_' + sensor_key + '_ld_' + str(idx), params=params)

    # opt.printResiduals()

    # ---------------------------------------
    # --- Compute the SPARSE MATRIX
    # ---------------------------------------
    print("Computing sparse matrix ... ")
    opt.computeSparseMatrix()
    # opt.printSparseMatrix()

    # ---------------------------------------
    # --- DEFINE THE VISUALIZATION FUNCTION
    # ---------------------------------------
    if args['view_optimization']:
        opt.setInternalVisualization(True)
    else:
        opt.setInternalVisualization(False)

    if args['ros_visualization']:
        print("Configuring visualization ... ")
        graphics = visualization.setupVisualization(dataset, args)
        # pp = pprint.PrettyPrinter(indent=4)
        # pp.pprint(dataset_graphics)
        opt.addDataModel('graphics', graphics)

        # opt.setVisualizationFunction(visualizationFunction, False, niterations=1, figures=[])
        opt.setVisualizationFunction(visualization.visualizationFunction, args['ros_visualization'], niterations=1,
                                     figures=[])

    # ---------------------------------------
    # --- Start Optimization
    # ---------------------------------------

    print('Initializing optimization ...')
    # opt.startOptimization(optimization_options={'ftol': 1e-4, 'xtol': 1e-4, 'gtol': 1e-4, 'diff_step': 1e-3,
    #                                             'x_scale': 'jac'})
    opt.startOptimization(optimization_options={'ftol': 1e-8, 'xtol': 1e-8, 'gtol': 1e-8, 'diff_step': 1e-3,
                                                'x_scale': 'jac'})
    # TODO diff step as None

    # print('\n-----------------')
    # opt.printParameters(opt.x0, text='Initial parameters')
    # print('\n')
    # opt.printParameters(opt.xf, text='Final parameters')

    # ---------------------------------------
    # --- Save updated JSON file
    # ---------------------------------------
    # Write json file with updated dataset
    print('Saving json file ...')
    createJSONFile(os.path.dirname(json_file) + '/results.json', dataset)
    print('File saved.')

    while True:
        opt.callObjectiveFunction()
        print(Fore.RED + 'Optimization finished, press ctrl-c to stop' + Style.RESET_ALL)

    # # ---------------------------------------
    # # --- Save updated xacro
    # # ---------------------------------------
    # # Cycle all sensors in calibration config, and for each replace the optimized transform in the original xacro
    # print('Save updated xacro file ...')
    # for sensor_key in dataset['calibration_config']['sensors']:
    #     child = dataset['calibration_config']['sensors'][sensor_key]['child_link']
    #     parent = dataset['calibration_config']['sensors'][sensor_key]['parent_link']
    #     transform_key = parent + '-' + child
    #
    #     trans = list(dataset['collections'][selected_collection_key]['transforms'][transform_key]['trans'])
    #     quat = list(dataset['collections'][selected_collection_key]['transforms'][transform_key]['quat'])
    #     found = False
    #
    #     for joint in xml_robot.joints:
    #         if joint.parent == parent and joint.child == child:
    #             found = True
    #             print('Found joint: ' + str(joint.name))
    #
    #             print('Replacing xyz = ' + str(joint.origin.xyz) + ' by ' + str(trans))
    #             joint.origin.xyz = trans
    #
    #             rpy = list(tf.transformations.euler_from_quaternion(quat, axes='sxyz'))
    #             print('Replacing rpy = ' + str(joint.origin.rpy) + ' by ' + str(rpy))
    #             joint.origin.rpy = rpy
    #             break
    #
    #     if not found:
    #         raise ValueError('Could not find transform ' + str(transform_key) + ' in /robot_description')
    #
    # # TODO remove hardcoded xacro name
    # outfile = rospkg.RosPack().get_path('atom_calibration') + '/calibrations/atlascar2/optimized.urdf.xacro'
    # with open(outfile, 'w') as out:
    #     print("Writing optimized urdf to " + str(outfile))
    #     out.write(URDF.to_xml_string(xml_robot))
    #
    # print('Xacro file saved.')


if __name__ == "__main__":
    main()
