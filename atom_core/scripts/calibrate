#!/usr/bin/env python
"""
Reads a set of data and labels from a group of sensors in a json file and calibrates the poses of these sensors.
"""

# stdlib
import os
import rospkg
import signal
import sys
import json
import sys
import argparse

from copy import deepcopy
from functools import partial
from urdf_parser_py.urdf import URDF

# 3rd-party
import rospy
import tf
import cv2
import numpy as np
from colorama import Fore, Style, Back

import OptimizationUtils.OptimizationUtils as OptimizationUtils
import OptimizationUtils.utilities as opt_utilities

# own packages
import atom_core.getters_and_setters as getters_and_setters
import atom_core.objective_function as objective_function
import atom_core.patterns as patterns
import atom_core.visualization as visualization

from atom_core.utilities import uriReader, execute, readXacroFile, write_pcd, read_pcd

# -------------------------------------------------------------------------------
# --- FUNCTIONS
# -------------------------------------------------------------------------------



def signal_handler(sig, frame):
    print('Stopping optimization (Ctrl+C pressed)')
    sys.exit(0)


def is_jsonable(x):
    try:
        json.dumps(x)
        return True
    except (TypeError, OverflowError):
        return False


def walk(node):
    for key, item in node.items():
        if isinstance(item, dict):
            walk(item)
        else:
            if isinstance(item, np.ndarray) and key == 'data':  # to avoid saving images in the json
                del node[key]

            elif isinstance(item, np.ndarray):
                node[key] = item.tolist()
            pass


def fitPlaneLTSQ(XYZ):
    (rows, cols) = XYZ.shape
    G = np.ones((rows, 3))
    G[:, 0] = XYZ[:, 0]  # X
    G[:, 1] = XYZ[:, 1]  # Y
    Z = XYZ[:, 2]
    (a, b, c), resid, rank, s = np.linalg.lstsq(G, Z)
    normal = (a, b, -1)
    nn = np.linalg.norm(normal)
    normal = normal / nn
    return (c, normal)


# Save to json file
def createJSONFile(output_file, input):
    D = deepcopy(input)
    walk(D)

    print("Saving the json output file to " + str(output_file) + ", please wait, it could take a while ...")
    f = open(output_file, 'w')
    json.encoder.FLOAT_REPR = lambda f: ("%.6f" % f)  # to get only four decimal places on the json file
    print >> f, json.dumps(D, indent=2, sort_keys=True)
    f.close()
    print("Completed.")


# -------------------------------------------------------------------------------
# --- MAIN
# -------------------------------------------------------------------------------
def main():
    # ---------------------------------------
    # --- Parse command line argument
    # ---------------------------------------
    signal.signal(signal.SIGINT, signal_handler)
    # print('Press Ctrl+C')
    # signal.pause()

    ap = argparse.ArgumentParser()
    ap = OptimizationUtils.addArguments(ap)  # OptimizationUtils arguments
    ap.add_argument("-json", "--json_file", help="Json file containing input dataset.", type=str, required=True)
    ap.add_argument("-rv", "--ros_visualization", help="Publish ros visualization markers.", action='store_true')
    ap.add_argument("-si", "--show_images", help="shows images for each camera", action='store_true', default=False)
    ap.add_argument("-oi", "--optimize_intrinsics", help="Adds camera instrinsics to the optimization",
                    action='store_true', default=False)
    ap.add_argument("-pof", "--profile_objective_function", help="Runs and prints a profile of the objective function, then exits.",
                    action='store_true', default=False)
    ap.add_argument("-sr", "--sample_residuals", help="Samples residuals", type=float, default=1)
    ap.add_argument("-od", "--optimize_distortion", help="Adds camera distortion parameters to the optimization",
                    action='store_true', default=False)
    ap.add_argument("-fec", "--four_extrema_corners", help="Uses only the four extrema corners for camera "
                                                           "optimizations, i.e. top left and right, bottom left and "
                                                           "right. This speeds up the optimization procedure (since "
                                                           "the number of residuals greatly reduces) but may result "
                                                           "in a less accurate calibration",
                    action='store_true', default=False)
    ap.add_argument("-uic", "--use_incomplete_collections",
                    help="Remove any collection which does not have a detection for all sensors.",
                    action='store_true', default=False)
    ap.add_argument("-rpd", "--remove_partial_detections",
                    help="Remove detected labels which are only partial. Used or the Charuco.",
                    action='store_true', default=False)
    ap.add_argument("-ssf", "--sensor_selection_function", default=None, type=lambda s: eval(s, globals()),
                    help='A string to be evaluated into a lambda function that receives a sensor name as input and '
                         'returns True or False to indicate if the sensor should be loaded (and used in the '
                         'optimization). The Syntax is lambda name: f(x), where f(x) is the function in python '
                         'language. Example: lambda name: name in ["left_laser", "frontal_camera"] , to load only '
                         'sensors left_laser and frontal_camera')
    ap.add_argument("-csf", "--collection_selection_function", default=None, type=lambda s: eval(s, globals()),
                    help='A string to be evaluated into a lambda function that receives a collection name as input and '
                         'returns True or False to indicate if the collection should be loaded (and used in the '
                         'optimization). The Syntax is lambda name: f(x), where f(x) is the function in python '
                         'language. Example: lambda name: int(name) > 5 , to load only collections 6, 7, and onward.')

    # Filter unwanted arguments.
    # Roslaunch adds two arguments (__name and __log) that break our parser
    arglist = [x for x in sys.argv[1:] if not x.startswith('__')]

    args = vars(ap.parse_args(args=arglist))
    print("\nArgument list=" + str(args) + '\n')

    # ---------------------------------------
    # --- Reading robot description file from param /robot_description
    # ---------------------------------------
    # rospy.loginfo('Reading /robot_description ros param')
    # xml_robot = URDF.from_parameter_server()  # needed to create the optimized xacro at the end of the optimization

    # ---------------------------------------
    # --- INITIALIZATION Read data from file
    # ---------------------------------------
    # Loads a json file containing the detections
    #
    # NOTE(eurico): I removed the URI reader because the argument is provided by the command line
    #   and our guide lines is to use environment variables, which the shell already expands.
    #   Furthermore, the URI resolver required an import from a `top-level` package which does
    #   not make sense in a `core` package.
    json_file = args['json_file']
    f = open(json_file, 'r')
    dataset = json.load(f)

    # Load images from files into memory. Images in the json file are stored in separate png files and in their place
    # a field "data_file" is saved with the path to the file. We must load the images from the disk.
    for collection_key, collection in dataset['collections'].items():
        for sensor_key, sensor in dataset['sensors'].items():
            if sensor['msg_type'] == 'Image':  # Load image.
                filename = os.path.dirname(json_file) + '/' + collection['data'][sensor_key]['data_file']
                collection['data'][sensor_key]['data'] = cv2.imread(filename)

    if not args['collection_selection_function'] is None:
        deleted = []
        for collection_key in dataset['collections'].keys():
            if not args['collection_selection_function'](collection_key):  # use the lambda expression csf
                deleted.append(collection_key)
                del dataset['collections'][collection_key]
        print("Deleted collections: " + str(deleted))

    if not args['use_incomplete_collections']:
        # Deleting collections where the pattern is not found by all sensors:
        for collection_key, collection in dataset['collections'].items():
            for sensor_key, sensor in dataset['sensors'].items():
                if not collection['labels'][sensor_key]['detected']:
                    print(Fore.RED + "Removing collection " + collection_key + ' -> pattern was not found in sensor ' +
                          sensor_key + ' (must be found in all sensors).' + Style.RESET_ALL)
                    del dataset['collections'][collection_key]
                    break

    if args['remove_partial_detections']:
        number_of_corners = int(dataset['calibration_config']['calibration_pattern']['dimension']['x']) * \
                            int(dataset['calibration_config']['calibration_pattern']['dimension']['y'])
        # Deleting labels in which not all corners are found:
        for collection_key, collection in dataset['collections'].items():
            for sensor_key, sensor in dataset['sensors'].items():
                if sensor['msg_type'] == 'Image' and collection['labels'][sensor_key]['detected']:
                    if not len(collection['labels'][sensor_key]['idxs']) == number_of_corners:
                        print(Fore.RED + 'Partial detection removed:' + Style.RESET_ALL + ' label from collection ' +
                              collection_key + ', sensor ' + sensor_key)
                        collection['labels'][sensor_key]['detected'] = False

    if not dataset['collections'].keys():
        raise ValueError('No collections were selected. Cannot optimize without collections. Please revise your '
                         'dataset and your collection selection function.')
    else:
        print("\nCollections studied:\n " + str(dataset['collections'].keys()))

    # ---------------------------------------
    # --- CREATE CHESSBOARD DATASET
    # ---------------------------------------
    dataset['patterns'] = patterns.createPatternLabels(args, dataset)

    # ---------------------------------------
    # --- FILTER SOME OF THE ELEMENTS LOADED, TO USE ONLY A SUBSET IN THE CALIBRATION
    # ---------------------------------------
    if not args['sensor_selection_function'] is None:
        deleted = []
        print(args['sensor_selection_function'])
        for sensor_key in dataset['sensors'].keys():
            if not args['sensor_selection_function'](sensor_key):  # use the lambda expression ssf
                deleted.append(sensor_key)
                del dataset['sensors'][sensor_key]
        print("Deleted sensors: " + str(deleted))

    print('Loaded dataset containing ' + str(len(dataset['sensors'].keys())) + ' sensors and ' + str(
        len(dataset['collections'].keys())) + ' collections.')

    # ---------------------------------------
    # --- DETECT EDGES IN THE LASER SCANS
    # ---------------------------------------
    for sensor_key, sensor in dataset['sensors'].items():
        if sensor['msg_type'] == 'LaserScan':  # only for lasers
            for collection_key, collection in dataset['collections'].items():
                idxs = collection['labels'][sensor_key]['idxs']
                edges = []  # a list of edges
                for i in range(0, len(idxs) - 1):
                    if (idxs[i + 1] - idxs[i]) != 1:
                        edges.append(i)
                        edges.append(i + 1)

                # Remove first (right most) and last (left most) edges, since these are often false edges.
                if len(edges) > 0:
                    edges.pop(0)  # remove the first element.
                if len(edges) > 0:
                    edges.pop()  # if the index is not given, then the last element is popped out and removed.
                collection['labels'][sensor_key]['edge_idxs'] = edges

    # ---------------------------------------
    # --- Detect corners in velodyne data
    # ---------------------------------------
    for sensor_key, sensor in dataset['sensors'].items():
        if sensor['msg_type'] == 'PointCloud2':  # only for 3D Lidars and RGBD cameras
            for collection_key, collection in dataset['collections'].items():
                import ros_numpy
                from scipy.spatial import distance

                # Read pointcloud from pcd file
                filename = os.path.dirname(json_file) + '/' + collection['data'][sensor_key]['data_file']
                frame_id = collection['data'][sensor_key]['header']['frame_id']
                cloud_msg = read_pcd(filename, frame_id, get_tf=False)
                collection['data'][sensor_key]['data'] = cloud_msg
                print(collection_key)

                # ------------------------------------------------------------------------------------------------
                # -------- Extract the labelled LiDAR points on the pattern
                # ------------------------------------------------------------------------------------------------
                idxs = collection['labels'][sensor_key]['idxs']
                pc = ros_numpy.numpify(cloud_msg)[idxs]
                points = np.zeros((pc.shape[0], 4))
                points[:, 0] = pc['x']
                points[:, 1] = pc['y']
                points[:, 2] = pc['z']
                points[:, 3] = 1
                collection['labels'][sensor_key]['labelled_points'] = []
                for idx in range(0, points.shape[0]):
                    collection['labels'][sensor_key]['labelled_points'].append(
                        {'x': points[idx, 0], 'y': points[idx, 1], 'z': points[idx, 2],
                         'w': points[idx, 3]})

                # - Cartesian to polar LiDAR points conversion
                import math
                points_sph = []
                for idx in range(points.shape[0]):
                    m_pt = points[idx, 0:3]
                    r = math.sqrt(m_pt[0] ** 2 + m_pt[1] ** 2 + m_pt[2] ** 2)
                    theta = math.acos(m_pt[2] / r)
                    phi = math.atan2(m_pt[1], m_pt[0])

                    m_pt_shp = [r, theta, phi]
                    points_sph.append(m_pt_shp)

                # - LiDAR beam clustering using the theta component
                points_sph = np.array(points_sph).transpose()
                thetas = points_sph[1, :].round(decimals=4)  # we round so that we can use the np.unique
                unique, indexes, inverse_indexes = np.unique(thetas, return_index=True, return_inverse=True)

                # - Find the extrema points using the maximum and minimum of the phi component for each cluster
                extrema_points = []
                middle_points = []
                for i in range(0, len(indexes)):
                    m_beam = np.where(inverse_indexes == i)

                    phis = points_sph[2, m_beam][0]
                    min_idx = np.argmin(phis)
                    max_idx = np.argmax(phis)

                    for phi in phis:
                        if not phi == np.min(phis) and not phi == np.max(phis):
                            idx = np.where(points_sph[2, :] == phi)[0][0]
                            middle_points.append(points[idx, :])

                    global_min_idx = np.where(points_sph[2, :] == phis[min_idx])[0][0]
                    global_max_idx = np.where(points_sph[2, :] == phis[max_idx])[0][0]

                    extrema_points.append(points[global_min_idx, :])
                    extrema_points.append(points[global_max_idx, :])

                # Save extrema points in a dictionary
                collection['labels'][sensor_key]['limit_points'] = []
                extrema_points = np.array(extrema_points)
                for idx in range(0, len(extrema_points)):
                    collection['labels'][sensor_key]['limit_points'].append(
                        {'x': extrema_points[idx, 0], 'y': extrema_points[idx, 1], 'z': extrema_points[idx, 2],
                         'w': extrema_points[idx, 3]})
                collection['labels'][sensor_key]['middle_points'] = []
                middle_points = np.array(middle_points)
                for idx in range(0, len(middle_points)):
                    collection['labels'][sensor_key]['middle_points'].append(
                        {'x': middle_points[idx, 0], 'y': middle_points[idx, 1], 'z': middle_points[idx, 2],
                         'w': middle_points[idx, 3]})
                # # - Save extrema points for optimization and visualization
                # collection['labels'][sensor_key]['limit_points'] = list(extrema_points)
                # # - Save middle points for optimization and visualization
                # collection['labels'][sensor_key]['middle_points'] = list(middle_points)

    # ---------------------------------------
    # --- SETUP OPTIMIZER: Create data models
    # ---------------------------------------
    opt = OptimizationUtils.Optimizer()
    opt.addDataModel('args', args)
    opt.addDataModel('dataset', dataset)

    cache = {}
    opt.addDataModel('cache', cache)

    # For the getters we only need to get one collection. Lets take the first key on the dictionary and always get that
    # transformation.
    selected_collection_key = dataset['collections'].keys()[0]

    # ---------------------------------------
    # --- SETUP OPTIMIZER: Add sensor parameters
    # ---------------------------------------
    # Each sensor will have a position (tx,ty,tz) and a rotation (r1,r2,r3)

    # Add parameters related to the sensors
    translation_delta = 0.2
    # TODO temporary placement of top_left_camera
    # for collection_key, collection in dataset['collections'].items():
    #     collection['transforms']['base_link-top_left_camera']['trans'] = [-1.48, 0.22, 1.35]
    # dataset['calibration_config']['anchored_sensor'] = 'left_camera'
    print('Anchored sensor is ' + Fore.GREEN + dataset['calibration_config'][
        'anchored_sensor'] + Style.RESET_ALL)

    anchored_sensor = dataset['calibration_config']['anchored_sensor']
    if anchored_sensor in dataset['sensors']:
        anchored_parent = dataset['sensors'][anchored_sensor]['calibration_parent']
        anchored_child = dataset['sensors'][anchored_sensor]['calibration_child']
        anchored_transform_key = opt_utilities.generateKey(anchored_parent, anchored_child)
    else:
        anchored_transform_key = ''  # not transform is anchored
    # TODO If we want and anchored sensor we should search (and fix) all the transforms in its chain that do are being optimized

    print('Creating parameters ...')
    # Steaming from the config json, we define a transform to be optimized for each sensor. It could happen that two
    # or more sensors define the same transform to be optimized (#120). To cope with this we first create a list of
    # transformations to be optimized and then compute the unique set of that list.
    transforms_set = set()
    for sensor_key, sensor in dataset['sensors'].items():
        transform_key = opt_utilities.generateKey(sensor['calibration_parent'], sensor['calibration_child'])
        transforms_set.add(transform_key)

    for transform_key in transforms_set:  # push six parameters for each transform to be optimized.
        initial_transform = getters_and_setters.getterTransform(dataset, transform_key=transform_key,
                                                                collection_name=collection_key)

        if transform_key == anchored_transform_key:
            bound_max = [x + sys.float_info.epsilon for x in initial_transform]
            bound_min = [x - sys.float_info.epsilon for x in initial_transform]
        else:
            bound_max = [+np.inf for x in initial_transform]
            bound_min = [-np.inf for x in initial_transform]

        opt.pushParamVector(group_name=transform_key, data_key='dataset',
                            getter=partial(getters_and_setters.getterTransform, transform_key=transform_key,
                                           collection_name=selected_collection_key),
                            setter=partial(getters_and_setters.setterTransform, transform_key=transform_key,
                                           collection_name=None),
                            suffix=['_x', '_y', '_z', '_r1', '_r2', '_r3'], bound_max=bound_max, bound_min=bound_min)

    # Intrinsics
    # TODO bound_min and max for intrinsics
    if args['optimize_intrinsics']:
        for sensor_key, sensor in dataset['sensors'].items():
            if sensor['msg_type'] == 'Image':  # if sensor is a camera add intrinsics
                opt.pushParamVector(group_name=str(sensor_key) + '_intrinsics', data_key='dataset',
                                    getter=partial(getters_and_setters.getterCameraIntrinsics, sensor_key=sensor_key),
                                    setter=partial(getters_and_setters.setterCameraIntrinsics, sensor_key=sensor_key),
                                    suffix=['_fx', '_fy', '_cx', '_cy', '_k1', '_k2', '_t1', '_t2', '_k3'])

    # TODO handle optimize_distortion

    # ---------------------------------------
    # --- SETUP OPTIMIZER: Add pattern(s) parameters
    # ---------------------------------------
    # Each Pattern will have the position (tx,ty,tz) and rotation (r1,r2,r3)

    if not dataset['calibration_config']['calibration_pattern']['fixed']:  # Pattern not fixed -------------------------
        # If pattern is not fixed there will be a transform for each collection. To tackle this reference link called
        # according to what is on the dataset['calibration_config']['calibration_pattern']['link'] is prepended with
        # a "c<collection_name>" appendix. This is done automatically for the collection['transforms'] when
        # publishing ROS, but we must add this to the parameter name.
        parent = dataset['calibration_config']['calibration_pattern']['parent_link']
        child = dataset['calibration_config']['calibration_pattern']['link']
        transform_key = opt_utilities.generateKey(parent, child)

        for collection_key, collection in dataset['collections'].items():  # iterate all collections

            # Set transform using the initial estimate of the transformations.
            initial_estimate = dataset['patterns']['transforms_initial'][collection_key]
            if not initial_estimate['detected'] or not parent == initial_estimate['parent'] or \
                    not child == initial_estimate['child']:
                raise ValueError('Cannot set initial estimate for pattern at collection ' + collection_key)

            collection['transforms'][transform_key] = {'parent': parent, 'child': child,
                                                       'trans': initial_estimate['trans'],
                                                       'quat': initial_estimate['quat']}

            # Finally push the six parameters to describe the patterns pose w.r.t its parent link:
            #   a) The Getter will pick up transform from the collection collection_key
            #   b) The Setter will received a transform value and a collection_key and copy the transform to that of the
            #   corresponding collection
            opt.pushParamVector(group_name='c' + collection_key + '_' + transform_key, data_key='dataset',
                                getter=partial(getters_and_setters.getterTransform, transform_key=transform_key,
                                               collection_name=collection_key),
                                setter=partial(getters_and_setters.setterTransform, transform_key=transform_key,
                                               collection_name=collection_key),
                                suffix=['_x', '_y', '_z', '_r1', '_r2', '_r3'])

    else:  # fixed pattern ---------------------------------------------------------------------------------------------
        # if pattern is fixed it will not be replicated for all collections , i.e. there will be a single
        # reference link called according to what is on the dataset['calibration_config']['calibration_pattern'][
        # 'link']
        parent = dataset['calibration_config']['calibration_pattern']['parent_link']
        child = dataset['calibration_config']['calibration_pattern']['link']
        transform_key = opt_utilities.generateKey(parent, child)

        # Set transform using the initial estimate of the transformations.
        initial_estimate = dataset['patterns']['transforms_initial'][collection_key]
        if not initial_estimate['detected'] or not parent == initial_estimate['parent'] or \
                not child == initial_estimate['child']:
            raise ValueError('Cannot set initial estimate for pattern at collection ' + collection_key)

        # The pattern is fixed but we have a replicated transform for each collection. Lets add those.
        for collection_key, collection in dataset['collections'].items():
            collection['transforms'][transform_key] = {'parent': parent, 'child': child,
                                                       'trans': initial_estimate['trans'],
                                                       'quat': initial_estimate['quat']}

        # Finally push the six parameters to describe the patterns pose w.r.t its parent link:
        #   a) The Getter will pick up the collection from one selected collection (it does not really matter which,
        #       since they are replicas);
        #   b) The Setter will received a transform value and copy that to all collection replicas, to ensure they
        #       all have the same value. This is done by setting  "collection_name=None".
        opt.pushParamVector(group_name=transform_key, data_key='dataset',
                            getter=partial(getters_and_setters.getterTransform, transform_key=transform_key,
                                           collection_name=selected_collection_key),
                            setter=partial(getters_and_setters.setterTransform, transform_key=transform_key,
                                           collection_name=None),
                            suffix=['_x', '_y', '_z', '_r1', '_r2', '_r3'])

    opt.printParameters()

    # ---------------------------------------
    # --- Define THE OBJECTIVE FUNCTION
    # ---------------------------------------
    opt.setObjectiveFunction(objective_function.objectiveFunction)

    # ---------------------------------------
    # --- Define THE RESIDUALS
    # ---------------------------------------
    # Each residual is computed after the sensor and the pattern of a collection. Thus, each error will be affected
    # by the parameters tx,ty,tz,r1,r2,r3 of the sensor and the pattern

    print("Creating residuals ... ")
    for collection_key, collection in dataset['collections'].items():
        for sensor_key, sensor in dataset['sensors'].items():
            if not collection['labels'][sensor_key]['detected']:  # if pattern not detected by sensor in collection
                continue

            # Sensor related parameters
            sensors_transform_key = opt_utilities.generateKey(sensor['calibration_parent'],
                                                              sensor['calibration_child'])
            params = opt.getParamsContainingPattern(sensors_transform_key)

            # Intrinsics parameters
            if sensor['msg_type'] == 'Image' and args['optimize_intrinsics']:
                params.extend(opt.getParamsContainingPattern(sensor_key + '_intrinsics'))

            # Pattern related parameters
            if dataset['calibration_config']['calibration_pattern']['fixed']:
                pattern_transform_key = opt_utilities.generateKey(
                    dataset['calibration_config']['calibration_pattern']['parent_link'],
                    dataset['calibration_config']['calibration_pattern']['link'])
            else:
                pattern_transform_key = 'c' + collection_key + '_' + opt_utilities.generateKey(
                    dataset['calibration_config']['calibration_pattern']['parent_link'],
                    dataset['calibration_config']['calibration_pattern']['link'])

            params.extend(opt.getParamsContainingPattern(pattern_transform_key))  # pattern related params

            if sensor['msg_type'] == 'Image':  # if sensor is a camera use four residuals

                # Compute step as a function of residual sampling factor
                step = int(1 / float(args['sample_residuals']))
                for idx in collection['labels'][sensor_key]['idxs'][::step]:  # using all pattern corners
                    rname = 'c' + str(collection_key) + '_' + str(sensor_key) + '_corner' + str(idx['id'])
                    opt.pushResidual(name=rname, params=params)

            elif sensor['msg_type'] == 'LaserScan':  # if sensor is a 2D lidar add two residuals
                # TODO Implement sampling here if relevant
                # Extrema points (longitudinal error)
                opt.pushResidual(name= 'c' + collection_key + '_' + sensor_key + '_eleft', params=params)
                opt.pushResidual(name= 'c' + collection_key + '_' + sensor_key + '_eright', params=params)

                # Inner points, use detection of edges (longitudinal error)
                for idx, _ in enumerate(collection['labels'][sensor_key]['edge_idxs']):
                    opt.pushResidual(name= 'c' + collection_key + '_' + sensor_key + '_inner_' + str(idx), params=params)

                # Laser beam (orthogonal error)
                for idx in range(0, len(collection['labels'][sensor_key]['idxs'])):
                    opt.pushResidual(name= 'c' + collection_key + '_' + sensor_key + '_beam_' + str(idx), params=params)

            elif sensor['msg_type'] == 'PointCloud2':  # if sensor is a 3D lidar add two types of residuals
                # Laser beam error
                step = int(1 / float(args['sample_residuals']))
                # for idx in range(0, len(collection['labels'][sensor_key]['middle_points']), step):
                for idx in range(0, len(collection['labels'][sensor_key]['idxs']), step):
                    opt.pushResidual(name='c' + collection_key + '_' + sensor_key + '_oe_' + str(idx), params=params)
                # Extrema displacement error
                for idx in range(0, len(collection['labels'][sensor_key]['limit_points'])):
                    opt.pushResidual(name='c' + collection_key + '_' + sensor_key + '_ld_' + str(idx), params=params)

            print('Adding residuals for sensor ' + sensor_key + ' with msg_type ' + sensor['msg_type'] +
                  ' affected by parameters:\n' + str(params))

    opt.printResiduals()

    # ---------------------------------------
    # --- Compute the SPARSE MATRIX
    # ---------------------------------------
    print("Computing sparse matrix ... ")
    opt.computeSparseMatrix()
    opt.printSparseMatrix()

    # ---------------------------------------
    # --- DEFINE THE VISUALIZATION FUNCTION
    # ---------------------------------------
    if args['view_optimization']:
        opt.setInternalVisualization(True)
    else:
        opt.setInternalVisualization(False)

    if args['ros_visualization']:
        print("Configuring visualization ... ")
        graphics = visualization.setupVisualization(dataset, args, selected_collection_key)
        opt.addDataModel('graphics', graphics)

        opt.setVisualizationFunction(visualization.visualizationFunction, args['ros_visualization'], niterations=1,
                                     figures=[])

    # ---------------------------------------
    # --- Profile Objective Function
    # ---------------------------------------
    if args['profile_objective_function']:
        from line_profiler import LineProfiler
        lp = LineProfiler()
        lp_wrapper = lp(objective_function.objectiveFunction)
        lp_wrapper(opt.data_models)
        print(Back.LIGHTYELLOW_EX + '\n\n' + Fore.RED + Style.BRIGHT + 'First call of Objective Function (no cache)'
              + '\n\n' + Back.RESET +Fore.RESET + Style.RESET_ALL )
        lp.print_stats()

        lp_wrapper(opt.data_models)
        print(Back.LIGHTYELLOW_EX + '\n\n' + Fore.RED + Style.BRIGHT + 'Second call of Objective Function (with cache)'
              + '\n\n' + Back.RESET +Fore.RESET + Style.RESET_ALL )
        lp.print_stats()
        exit(0)

    # ---------------------------------------
    # --- Start Optimization
    # ---------------------------------------

    print('Initializing optimization ...')

    # TODO diff step as None
    # options = {'ftol': 1e-6, 'xtol': 1e-6, 'gtol': 1e-6, 'diff_step': 1e-4, 'x_scale': 'jac'}
    # options={'ftol': 1e-6, 'xtol': 1e-6, 'gtol': 1e-6, 'diff_step': 1e-3, 'x_scale': 'jac'}
    # options={'ftol': 1e-6, 'xtol': 1e-6, 'gtol': 1e-6}
    # options = {'ftol': 1e-6, 'xtol': 1e-6, 'gtol': 1e-6, 'diff_step': 1e-4, 'x_scale': 'jac', 'max_nfev': 1}
    # options = {'ftol': 1e-6, 'xtol': 1e-6, 'gtol': 1e-6, 'diff_step': None, 'x_scale': 'jac'}
    options = {'ftol': 1e-6, 'xtol': 1e-6, 'gtol': 1e-6, 'diff_step': None, 'max_nfev': 5}

    opt.startOptimization(optimization_options=options)

    # print('\n-----------------')
    # opt.printParameters(opt.x0, text='Initial parameters')
    # print('\n')
    # opt.printParameters(opt.xf, text='Final parameters')

    # ---------------------------------------
    # --- Save updated JSON file
    # ---------------------------------------
    createJSONFile(os.path.dirname(json_file) + '/atom_calibration.json', dataset)

    # while True:
    #     opt.callObjectiveFunction()
    #     print(Fore.RED + 'Optimization finished, press ctrl-c to stop' + Style.RESET_ALL)

    # ---------------------------------------
    # --- Save updated xacro
    # ---------------------------------------
    # Cycle all sensors in calibration config, and for each replace the optimized transform in the original xacro
    # Parse xacro description file
    description_file, _, _ = uriReader(dataset['calibration_config']['description_file'])
    rospy.loginfo('Reading description file ' + description_file + '...')
    xml_robot = readXacroFile(description_file)

    for sensor_key in dataset['calibration_config']['sensors']:
        child = dataset['calibration_config']['sensors'][sensor_key]['child_link']
        parent = dataset['calibration_config']['sensors'][sensor_key]['parent_link']
        transform_key = opt_utilities.generateKey(parent, child)

        trans = list(dataset['collections'][selected_collection_key]['transforms'][transform_key]['trans'])
        quat = list(dataset['collections'][selected_collection_key]['transforms'][transform_key]['quat'])
        found = False

        for joint in xml_robot.joints:
            if joint.parent == parent and joint.child == child:
                found = True
                print('Found joint: ' + str(joint.name))

                print('Replacing xyz = ' + str(joint.origin.xyz) + ' by ' + str(trans))
                joint.origin.xyz = trans

                rpy = list(tf.transformations.euler_from_quaternion(quat, axes='sxyz'))
                print('Replacing rpy = ' + str(joint.origin.rpy) + ' by ' + str(rpy))
                joint.origin.rpy = rpy
                break

        if not found:
            raise ValueError('Could not find transform ' + str(transform_key) + ' in ' + description_file)

    # TODO find a way to find the <your_robot>_calibration path. For now write a xacro to /tmp
    # outfile = os.path.dirname(description_file) + '/optimized.urdf.xacro'
    outfile = '/tmp/optimized.urdf.xacro'
    with open(outfile, 'w') as out:
        out.write(URDF.to_xml_string(xml_robot))

    print('Optimized xacro file saved to '+ str(outfile) + ' . You can use it as a ROS robot_description.')

if __name__ == "__main__":
    main()
