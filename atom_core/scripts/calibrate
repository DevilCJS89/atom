#!/usr/bin/env python
"""
Reads a set of data and labels from a group of sensors in a json file and calibrates the poses of these sensors.
"""

# stdlib
import os
import signal
import sys
import json
import sys
import argparse

from copy import deepcopy
from functools import partial

# 3rd-party
import tf
import cv2
import numpy as np
from colorama import Fore, Style

import OptimizationUtils.OptimizationUtils as OptimizationUtils
import OptimizationUtils.utilities as opt_utilities

# own packages
import atom_core.getters_and_setters as getters_and_setters
import atom_core.objective_function as objective_function
import atom_core.patterns as patterns
import atom_core.visualization as visualization


# -------------------------------------------------------------------------------
# --- FUNCTIONS
# -------------------------------------------------------------------------------
def signal_handler(sig, frame):
    print('Stopping optimization (Ctrl+C pressed)')
    sys.exit(0)


def is_jsonable(x):
    try:
        json.dumps(x)
        return True
    except (TypeError, OverflowError):
        return False


def walk(node):
    for key, item in node.items():
        if isinstance(item, dict):
            walk(item)
        else:
            if isinstance(item, np.ndarray) and key == 'data':  # to avoid saving images in the json
                del node[key]

            elif isinstance(item, np.ndarray):
                node[key] = item.tolist()
            pass


def fitPlaneLTSQ(XYZ):
    (rows, cols) = XYZ.shape
    G = np.ones((rows, 3))
    G[:, 0] = XYZ[:, 0]  # X
    G[:, 1] = XYZ[:, 1]  # Y
    Z = XYZ[:, 2]
    (a, b, c), resid, rank, s = np.linalg.lstsq(G, Z)
    normal = (a, b, -1)
    nn = np.linalg.norm(normal)
    normal = normal / nn
    return (c, normal)


# Save to json file
def createJSONFile(output_file, input):
    D = deepcopy(input)

    walk(D)

    print("Saving the json output file to " + str(output_file) + ", please wait, it could take a while ...")
    f = open(output_file, 'w')
    json.encoder.FLOAT_REPR = lambda f: ("%.6f" % f)  # to get only four decimal places on the json file
    print >> f, json.dumps(D, indent=2, sort_keys=True)
    f.close()
    print("Completed.")


# -------------------------------------------------------------------------------
# --- MAIN
# -------------------------------------------------------------------------------
def main():
    # ---------------------------------------
    # --- Parse command line argument
    # ---------------------------------------
    signal.signal(signal.SIGINT, signal_handler)
    # print('Press Ctrl+C')
    # signal.pause()

    ap = argparse.ArgumentParser()
    ap = OptimizationUtils.addArguments(ap)  # OptimizationUtils arguments
    ap.add_argument("-json", "--json_file", help="Json file containing input dataset.", type=str, required=True)
    ap.add_argument("-rv", "--ros_visualization", help="Publish ros visualization markers.", action='store_true')
    ap.add_argument("-si", "--show_images", help="shows images for each camera", action='store_true', default=False)
    ap.add_argument("-oi", "--optimize_intrinsics", help="Adds camera instrinsics to the optimization",
                    action='store_true', default=False)
    ap.add_argument("-od", "--optimize_distortion", help="Adds camera distortion parameters to the optimization",
                    action='store_true', default=False)
    ap.add_argument("-fec", "--four_extrema_corners", help="Uses only the four extrema corners for camera "
                                                           "optimizations, i.e. top left and right, bottom left and "
                                                           "right. This speeds up the optimization procedure (since "
                                                           "the number of residuals greatly reduces) but may result "
                                                           "in a less accurate calibration",
                    action='store_true', default=False)
    ap.add_argument("-uic", "--use_incomplete_collections",
                    help="Remove any collection which does not have a detection for all sensors.",
                    action='store_true', default=False)
    ap.add_argument("-rpd", "--remove_partial_detections",
                    help="Remove detected labels which are only partial. Used or the Charuco.",
                    action='store_true', default=False)
    ap.add_argument("-ssf", "--sensor_selection_function", default=None, type=lambda s: eval(s, globals()),
                    help='A string to be evaluated into a lambda function that receives a sensor name as input and '
                         'returns True or False to indicate if the sensor should be loaded (and used in the '
                         'optimization). The Syntax is lambda name: f(x), where f(x) is the function in python '
                         'language. Example: lambda name: name in ["left_laser", "frontal_camera"] , to load only '
                         'sensors left_laser and frontal_camera')
    ap.add_argument("-csf", "--collection_selection_function", default=None, type=lambda s: eval(s, globals()),
                    help='A string to be evaluated into a lambda function that receives a collection name as input and '
                         'returns True or False to indicate if the collection should be loaded (and used in the '
                         'optimization). The Syntax is lambda name: f(x), where f(x) is the function in python '
                         'language. Example: lambda name: int(name) > 5 , to load only collections 6, 7, and onward.')

    # Filter unwanted arguments.
    # Roslaunch adds two arguments (__name and __log) that break our parser
    arglist = [x for x in sys.argv[1:] if not x.startswith('__')]

    args = vars(ap.parse_args(args=arglist))
    print("\nArgument list=" + str(args) + '\n')

    # ---------------------------------------
    # --- Reading robot description file from param /robot_description
    # ---------------------------------------
    # rospy.loginfo('Reading /robot_description ros param')
    # xml_robot = URDF.from_parameter_server()  # needed to create the optimized xacro at the end of the optimization

    # ---------------------------------------
    # --- INITIALIZATION Read data from file
    # ---------------------------------------
    # Loads a json file containing the detections
    #
    # NOTE(eurico): I removed the URI reader because the argument is provided by the command line
    #   and our guide lines is to use environment variables, which the shell already expands.
    #   Furthermore, the URI resolver required an import from a `top-level` package which does
    #   not make sense in a `core` package.
    json_file = args['json_file']
    f = open(json_file, 'r')
    dataset = json.load(f)

    # Load images from files into memory. Images in the json file are stored in separate png files and in their place
    # a field "data_file" is saved with the path to the file. We must load the images from the disk.
    for collection_key, collection in dataset['collections'].items():
        for sensor_key, sensor in dataset['sensors'].items():
            if not sensor['msg_type'] == 'Image':  # nothing to do here.
                continue

            filename = os.path.dirname(json_file) + '/' + collection['data'][sensor_key]['data_file']
            collection['data'][sensor_key]['data'] = cv2.imread(filename)

    if not args['collection_selection_function'] is None:
        deleted = []
        for collection_key in dataset['collections'].keys():
            if not args['collection_selection_function'](collection_key):  # use the lambda expression csf
                deleted.append(collection_key)
                del dataset['collections'][collection_key]
        print("Deleted collections: " + str(deleted))

    if not args['use_incomplete_collections']:
        # Deleting collections where the pattern is not found by all sensors:
        for collection_key, collection in dataset['collections'].items():
            for sensor_key, sensor in dataset['sensors'].items():
                if not collection['labels'][sensor_key]['detected']:
                    print(Fore.RED + "Removing collection " + collection_key + ' -> pattern was not found in sensor ' +
                          sensor_key + ' (must be found in all sensors).' + Style.RESET_ALL)
                    del dataset['collections'][collection_key]
                    break

    if args['remove_partial_detections']:
        number_of_corners = int(dataset['calibration_config']['calibration_pattern']['dimension']['x']) * \
                            int(dataset['calibration_config']['calibration_pattern']['dimension']['y'])
        # Deleting labels in which not all corners are found:
        for collection_key, collection in dataset['collections'].items():
            for sensor_key, sensor in dataset['sensors'].items():
                if sensor['msg_type'] == 'Image' and collection['labels'][sensor_key]['detected']:
                    if not len(collection['labels'][sensor_key]['idxs']) == number_of_corners:
                        print(Fore.RED + 'Partial detection removed:' + Style.RESET_ALL + ' label from collection ' +
                              collection_key + ', sensor ' + sensor_key)
                        collection['labels'][sensor_key]['detected'] = False

    print("\nCollections studied:\n " + str(dataset['collections'].keys()))

    # ---------------------------------------
    # --- CREATE CHESSBOARD DATASET
    # ---------------------------------------
    dataset['patterns'] = patterns.createPatternLabels(args, dataset)

    # ---------------------------------------
    # --- FILTER SOME OF THE ELEMENTS LOADED, TO USE ONLY A SUBSET IN THE CALIBRATION
    # ---------------------------------------
    if not args['sensor_selection_function'] is None:
        deleted = []
        print(args['sensor_selection_function'])
        for sensor_key in dataset['sensors'].keys():
            if not args['sensor_selection_function'](sensor_key):  # use the lambda expression ssf
                deleted.append(sensor_key)
                del dataset['sensors'][sensor_key]
        print("Deleted sensors: " + str(deleted))

    print('Loaded dataset containing ' + str(len(dataset['sensors'].keys())) + ' sensors and ' + str(
        len(dataset['collections'].keys())) + ' collections.')

    # ---------------------------------------
    # --- DETECT EDGES IN THE LASER SCANS
    # ---------------------------------------
    for sensor_key, sensor in dataset['sensors'].items():
        if sensor['msg_type'] == 'LaserScan':  # only for lasers
            for collection_key, collection in dataset['collections'].items():
                idxs = collection['labels'][sensor_key]['idxs']
                edges = []  # a list of edges
                for i in range(0, len(idxs) - 1):
                    if (idxs[i + 1] - idxs[i]) != 1:
                        edges.append(i)
                        edges.append(i + 1)

                # Remove first (right most) and last (left most) edges, since these are often false edges.
                if len(edges) > 0:
                    edges.pop(0)  # remove the first element.
                if len(edges) > 0:
                    edges.pop()  # if the index is not given, then the last element is popped out and removed.
                collection['labels'][sensor_key]['edge_idxs'] = edges

    # ---------------------------------------
    # --- Detect corners in velodyne data
    # ---------------------------------------
    for sensor_key, sensor in dataset['sensors'].items():
        if sensor['msg_type'] == 'PointCloud2':  # only for 3D Lidars and RGBD cameras
            for collection_key, collection in dataset['collections'].items():
                import ros_numpy
                from scipy.spatial import distance
                from rospy_message_converter import message_converter

                # Convert 3D cloud data on .json dictionary to ROS message type
                cloud_msg = message_converter.convert_dictionary_to_ros_message("sensor_msgs/PointCloud2",
                                                                                collection['data'][sensor_key])

                # ------------------------------------------------------------------------------------------------
                # -------- Extract the labelled LiDAR points on the pattern
                # ------------------------------------------------------------------------------------------------
                idxs = collection['labels'][sensor_key]['idxs']
                pc = ros_numpy.numpify(cloud_msg)[idxs]
                points = np.zeros((pc.shape[0], 4))
                points[:, 0] = pc['x']
                points[:, 1] = pc['y']
                points[:, 2] = pc['z']
                points[:, 3] = 1
                collection['labels'][sensor_key]['labelled_points'] = points

                # ------------------------------------------------------------------------------------------------
                # -------- Convert cloud labelled points to 2D pattern frame
                # ------------------------------------------------------------------------------------------------
                c, normal = fitPlaneLTSQ(points[:, 0:3])  # Fit plane to points
                plane_a, plane_b, plane_c = normal[0], normal[1], normal[2]
                point = np.array([0.0, 0.0, c])
                plane_d = -point.dot(normal)

                # Define a point in the plane which is the first of the list, projected to the plane
                # use average y and z values (since velodyne uses x forward) to put origin on the center of the pattern
                pt_avg = np.average(points[:, 0:3], axis=0)

                y, z = pt_avg[1], pt_avg[2]
                x = (- plane_b * y - plane_c * z - plane_d) / plane_a
                p0 = np.array([x, y, z])

                # Define a second point. We use the first point from the list of points, and project it to the plane
                idx = 0
                y, z = points[idx, 1], points[idx, 2]
                x = (- plane_b * y - plane_c * z - plane_d) / plane_a
                p1 = np.array([x, y, z])

                # Define a rotation matrix, and the n (direction vector along the x axis), s and a vectors:
                #        n    s    a
                # R = [ r11  r12  r13 ]
                #     [ r21  r22  r23 ]
                #     [ r31  r32  r33 ]
                n = p1 - p0  # semi-arbitrary x-axis: only constraint is that it is coplanar with the plane. This is
                # ensured since both p0 and p1 lie on the plane
                a = normal  # z axis must have the direction normal to the plane
                s = np.cross(a, n)  # y axis is the cross product of z axis per the x axis

                def getUnitVector(v):
                    return v / np.linalg.norm(v)

                n, s, a = getUnitVector(n), getUnitVector(s), getUnitVector(a)
                # - Compute homogeneous transformation from velodyne to local reference system
                T = np.zeros((4, 4), np.float32)
                R = np.zeros((3, 3), np.float32)
                R[:, 0] = n
                R[:, 1] = s
                R[:, 2] = a
                trans = p0  # we use on of the points in the plane to define the translation
                T[0:3, 0:3] = R
                T[0:3, 3] = trans
                T[3, 3] = 1
                # - Convert to ros tf and save in transforms dictionary
                euler = tf.transformations.euler_from_matrix(R)
                quat = tf.transformations.quaternion_from_euler(euler[0], euler[1], euler[2])
                parent = sensor['parent']
                child = 'pattern_link_from_' + parent
                key = opt_utilities.generateKey(parent, child)
                collection['transforms'][key] = {'child': child, 'parent': parent, 'quat': list(quat),
                                                 'trans': list(trans)}

                # - Convert velodyne points into the 2D pattern reference frame
                pts_in_pattern = np.dot(np.linalg.inv(T), points.transpose())
                pts_in_pattern = pts_in_pattern.transpose()[:, 0:2]

                # ------------------------------------------------------------------------------------------------
                # -------- Find extrema points using circle radius fitting
                # ------------------------------------------------------------------------------------------------
                # CONFIGURATIONS
                start_angle = 0.  # start angle increment in 0 radians
                end_angle = 2. * np.pi  # end the angle increment when we complete the whole circle
                angle_step = 0.05 * np.pi / 180.  # angle step in radians
                # Compute circle radius using the higher distance between two points in the array
                pts_copy_a = list(pts_in_pattern)
                pts_copy_b = list(pts_in_pattern)
                radius = np.max(distance.cdist(pts_copy_a, pts_copy_b, 'euclidean')) / 2. + 0.1
                # Compute a circle of points outside the pattern and find the pattern point closer to each source point
                extrema_points = []
                seed_point = np.average(pts_in_pattern, axis=0)
                th = start_angle
                print('Computing 3D cloud pattern limit points for collection ' + str(collection_key))
                while th < end_angle:
                    # Compute the source points out of the pattern bounds
                    source_pt = [[seed_point[0] + radius * np.cos(th),
                                  seed_point[1] + radius * np.sin(th)]]
                    # Compute point that is more distant from the seed point
                    c_idx = np.argmin(distance.cdist(source_pt, pts_in_pattern[:, :], 'euclidean'))
                    extrema_points.append(points[c_idx, :])

                    th += angle_step
                    # ----------------

                # Delete duplicates from extrema points array
                unique_extrema_points = list(set(map(tuple, extrema_points)))
                collection['labels'][sensor_key]['limit_points'] = unique_extrema_points

    # ---------------------------------------
    # --- SETUP OPTIMIZER: Create data models
    # ---------------------------------------
    opt = OptimizationUtils.Optimizer()
    opt.addDataModel('args', args)
    opt.addDataModel('dataset', dataset)

    # For the getters we only need to get one collection. Lets take the first key on the dictionary and always get that
    # transformation.
    selected_collection_key = dataset['collections'].keys()[0]

    # ---------------------------------------
    # --- SETUP OPTIMIZER: Add sensor parameters
    # ---------------------------------------
    # Each sensor will have a position (tx,ty,tz) and a rotation (r1,r2,r3)

    # Add parameters related to the sensors
    translation_delta = 0.2
    # TODO temporary placement of top_left_camera
    # for collection_key, collection in dataset['collections'].items():
    #     collection['transforms']['base_link-top_left_camera']['trans'] = [-1.48, 0.22, 1.35]
    # dataset['calibration_config']['anchored_sensor'] = 'right_laser'
    print('Anchored sensor is ' + Fore.GREEN + dataset['calibration_config'][
        'anchored_sensor'] + Style.RESET_ALL)

    anchored_sensor = dataset['calibration_config']['anchored_sensor']
    anchored_parent = dataset['sensors'][anchored_sensor]['calibration_parent']
    anchored_child = dataset['sensors'][anchored_sensor]['calibration_child']
    anchored_transform_key = opt_utilities.generateKey(anchored_parent, anchored_child)
    # TODO If we want and anchored sensor we should search (and fix) all the transforms in its chain that do are being optimized

    print('Creating parameters ...')
    # Steaming from the config json, we define a transform to be optimized for each sensor. It could happen that two
    # or more sensors define the same transform to be optimized (#120). To cope with this we first create a list of
    # transformations to be optimized and then compute the unique set of that list.
    transforms_set = set()
    for sensor_key, sensor in dataset['sensors'].items():
        transform_key = opt_utilities.generateKey(sensor['calibration_parent'], sensor['calibration_child'])
        transforms_set.add(transform_key)

    for transform_key in transforms_set:  # push six parameters for each transform to be optimized.
        initial_transform = getters_and_setters.getterTransform(dataset, transform_key=transform_key,
                                                                collection_name=collection_key)

        if transform_key == anchored_transform_key:
            bound_max = [x + sys.float_info.epsilon for x in initial_transform]
            bound_min = [x - sys.float_info.epsilon for x in initial_transform]
        else:
            bound_max = [+np.inf for x in initial_transform]
            bound_min = [-np.inf for x in initial_transform]

        opt.pushParamVector(group_name=transform_key, data_key='dataset',
                            getter=partial(getters_and_setters.getterTransform, transform_key=transform_key,
                                           collection_name=selected_collection_key),
                            setter=partial(getters_and_setters.setterTransform, transform_key=transform_key,
                                           collection_name=None),
                            suffix=['_x', '_y', '_z', '_r1', '_r2', '_r3'], bound_max=bound_max, bound_min=bound_min)

    # Intrinsics
    # TODO bound_min and max for intrinsics
    if args['optimize_intrinsics']:
        for sensor_key, sensor in dataset['sensors'].items():
            if sensor['msg_type'] == 'Image':  # if sensor is a camera add intrinsics
                opt.pushParamVector(group_name=str(sensor_key) + '_intrinsics', data_key='dataset',
                                    getter=partial(getters_and_setters.getterCameraPMatrix, sensor_key=sensor_key),
                                    setter=partial(getters_and_setters.setterCameraPMatrix, sensor_key=sensor_key),
                                    suffix=['_fx_p', '_fy_p', '_cx_p', '_cy_p'])
    # TODO handle optimize_distortion

    # ---------------------------------------
    # --- SETUP OPTIMIZER: Add pattern(s) parameters
    # ---------------------------------------
    # Each Pattern will have the position (tx,ty,tz) and rotation (r1,r2,r3)

    if not dataset['calibration_config']['calibration_pattern']['fixed']:  # Pattern not fixed -------------------------
        # If pattern is not fixed there will be a transform for each collection. To tackle this reference link called
        # according to what is on the dataset['calibration_config']['calibration_pattern']['link'] is prepended with
        # a "c<collection_name>" appendix. This is done automatically for the collection['transforms'] when
        # publishing ROS, but we must add this to the parameter name.
        parent = dataset['calibration_config']['calibration_pattern']['parent_link']
        child = dataset['calibration_config']['calibration_pattern']['link']
        transform_key = opt_utilities.generateKey(parent, child)

        for collection_key, collection in dataset['collections'].items():  # iterate all collections

            # Set transform using the initial estimate of the transformations.
            initial_estimate = dataset['patterns']['transforms_initial'][collection_key]
            if not initial_estimate['detected'] or not parent == initial_estimate['parent'] or \
                    not child == initial_estimate['child']:
                raise ValueError('Cannot set initial estimate for pattern at collection ' + collection_key)

            collection['transforms'][transform_key] = {'parent': parent, 'child': child,
                                                       'trans': initial_estimate['trans'],
                                                       'quat': initial_estimate['quat']}

            # Finally push the six parameters to describe the patterns pose w.r.t its parent link:
            #   a) The Getter will pick up transform from the collection collection_key
            #   b) The Setter will received a transform value and a collection_key and copy the transform to that of the
            #   corresponding collection
            opt.pushParamVector(group_name='c' + collection_key + '_' + transform_key, data_key='dataset',
                                getter=partial(getters_and_setters.getterTransform, transform_key=transform_key,
                                               collection_name=collection_key),
                                setter=partial(getters_and_setters.setterTransform, transform_key=transform_key,
                                               collection_name=collection_key),
                                suffix=['_x', '_y', '_z', '_r1', '_r2', '_r3'])

    else:  # fixed pattern ---------------------------------------------------------------------------------------------
        # if pattern is fixed it will not be replicated for all collections , i.e. there will be a single
        # reference link called according to what is on the dataset['calibration_config']['calibration_pattern'][
        # 'link']
        parent = dataset['calibration_config']['calibration_pattern']['parent_link']
        child = dataset['calibration_config']['calibration_pattern']['link']
        transform_key = opt_utilities.generateKey(parent, child)

        # Set transform using the initial estimate of the transformations.
        initial_estimate = dataset['patterns']['transforms_initial'][collection_key]
        if not initial_estimate['detected'] or not parent == initial_estimate['parent'] or \
                not child == initial_estimate['child']:
            raise ValueError('Cannot set initial estimate for pattern at collection ' + collection_key)

        # The pattern is fixed but we have a replicated transform for each collection. Lets add those.
        for collection_key, collection in dataset['collections'].items():
            collection['transforms'][transform_key] = {'parent': parent, 'child': child,
                                                       'trans': initial_estimate['trans'],
                                                       'quat': initial_estimate['quat']}

        # Finally push the six parameters to describe the patterns pose w.r.t its parent link:
        #   a) The Getter will pick up the collection from one selected collection (it does not really matter which,
        #       since they are replicas);
        #   b) The Setter will received a transform value and copy that to all collection replicas, to ensure they
        #       all have the same value. This is done by setting  "collection_name=None".
        opt.pushParamVector(group_name=transform_key, data_key='dataset',
                            getter=partial(getters_and_setters.getterTransform, transform_key=transform_key,
                                           collection_name=selected_collection_key),
                            setter=partial(getters_and_setters.setterTransform, transform_key=transform_key,
                                           collection_name=None),
                            suffix=['_x', '_y', '_z', '_r1', '_r2', '_r3'])

    opt.printParameters()

    # ---------------------------------------
    # --- Define THE OBJECTIVE FUNCTION
    # ---------------------------------------
    opt.setObjectiveFunction(objective_function.objectiveFunction)

    # ---------------------------------------
    # --- Define THE RESIDUALS
    # ---------------------------------------
    # Each residual is computed after the sensor and the pattern of a collection. Thus, each error will be affected
    # by the parameters tx,ty,tz,r1,r2,r3 of the sensor and the pattern

    print("Creating residuals ... ")
    for collection_key, collection in dataset['collections'].items():
        for sensor_key, sensor in dataset['sensors'].items():
            if not collection['labels'][sensor_key]['detected']:  # if pattern not detected by sensor in collection
                continue

            # Sensor related parameters
            sensors_transform_key = opt_utilities.generateKey(sensor['calibration_parent'],
                                                              sensor['calibration_child'])
            params = opt.getParamsContainingPattern(sensors_transform_key)

            # Intrinsics parameters
            if sensor['msg_type'] == 'Image' and args['optimize_intrinsics']:
                params.extend(opt.getParamsContainingPattern(sensor_key + '_intrinsics'))

            # Pattern related parameters
            if dataset['calibration_config']['calibration_pattern']['fixed']:
                pattern_transform_key = opt_utilities.generateKey(
                    dataset['calibration_config']['calibration_pattern']['parent_link'],
                    dataset['calibration_config']['calibration_pattern']['link'])
            else:
                pattern_transform_key = 'c' + collection_key + '_' + opt_utilities.generateKey(
                    dataset['calibration_config']['calibration_pattern']['parent_link'],
                    dataset['calibration_config']['calibration_pattern']['link'])

            params.extend(opt.getParamsContainingPattern(pattern_transform_key))  # pattern related params

            if sensor['msg_type'] == 'Image':  # if sensor is a camera use four residuals

                for idx in collection['labels'][sensor_key]['idxs']:  # using all pattern corners
                    rname = 'c' + str(collection_key) + '_' + str(sensor_key) + '_corner' + str(idx['id'])
                    opt.pushResidual(name=rname, params=params)

            elif sensor['msg_type'] == 'LaserScan':  # if sensor is a 2D lidar add two residuals
                # Extrema points (longitudinal error)
                opt.pushResidual(name=collection_key + '_' + sensor_key + '_eleft', params=params)
                opt.pushResidual(name=collection_key + '_' + sensor_key + '_eright', params=params)

                # Inner points, use detection of edges (longitudinal error)
                for idx, _ in enumerate(collection['labels'][sensor_key]['edge_idxs']):
                    opt.pushResidual(name=collection_key + '_' + sensor_key + '_inner_' + str(idx), params=params)

                # Laser beam (orthogonal error)
                for idx in range(0, len(collection['labels'][sensor_key]['idxs'])):
                    opt.pushResidual(name=collection_key + '_' + sensor_key + '_beam_' + str(idx), params=params)

            elif sensor['msg_type'] == 'PointCloud2':  # if sensor is a 3D lidar add two types of residuals
                # Laser beam error
                for idx in range(0, len(collection['labels'][sensor_key]['idxs'])):
                    opt.pushResidual(name=collection_key + '_' + sensor_key + '_oe_' + str(idx), params=params)
                # Extrema displacement error
                for idx in range(0, len(collection['labels'][sensor_key]['limit_points'])):
                    opt.pushResidual(name=collection_key + '_' + sensor_key + '_ld_' + str(idx), params=params)

            print('Adding residuals for sensor ' + sensor_key + ' with msg_type ' + sensor['msg_type'] +
                  ' affected by parameters:\n' + str(params))

    opt.printResiduals()

    # ---------------------------------------
    # --- Compute the SPARSE MATRIX
    # ---------------------------------------
    print("Computing sparse matrix ... ")
    opt.computeSparseMatrix()
    opt.printSparseMatrix()

    # ---------------------------------------
    # --- DEFINE THE VISUALIZATION FUNCTION
    # ---------------------------------------
    if args['view_optimization']:
        opt.setInternalVisualization(True)
    else:
        opt.setInternalVisualization(False)

    if args['ros_visualization']:
        print("Configuring visualization ... ")
        graphics = visualization.setupVisualization(dataset, args)
        opt.addDataModel('graphics', graphics)

        opt.setVisualizationFunction(visualization.visualizationFunction, args['ros_visualization'], niterations=1,
                                     figures=[])

    # ---------------------------------------
    # --- Start Optimization
    # ---------------------------------------

    print('Initializing optimization ...')
    # opt.startOptimization(optimization_options={'ftol': 1e-8, 'xtol': 1e-8, 'gtol': 1e-8, 'diff_step': 1e-4, 'x_scale':'jac'})
    # opt.startOptimization(optimization_options={'ftol': 1e-6, 'xtol': 1e-6, 'gtol': 1e-6, 'diff_step': 1e-4, 'x_scale':'jac'})
    opt.startOptimization(optimization_options={'ftol': 1e-6, 'xtol': 1e-6, 'gtol': 1e-6})
    # TODO diff step as None

    # print('\n-----------------')
    # opt.printParameters(opt.x0, text='Initial parameters')
    # print('\n')
    # opt.printParameters(opt.xf, text='Final parameters')

    # ---------------------------------------
    # --- Save updated JSON file
    # ---------------------------------------
    # Write json file with updated dataset
    print('Saving json file ...')
    createJSONFile(os.path.dirname(json_file) + '/results.json', dataset)
    print('File saved.')

    while True:
        opt.callObjectiveFunction()
        print(Fore.RED + 'Optimization finished, press ctrl-c to stop' + Style.RESET_ALL)

    # # ---------------------------------------
    # # --- Save updated xacro
    # # ---------------------------------------
    # # Cycle all sensors in calibration config, and for each replace the optimized transform in the original xacro
    # print('Save updated xacro file ...')
    # for sensor_key in dataset['calibration_config']['sensors']:
    #     child = dataset['calibration_config']['sensors'][sensor_key]['child_link']
    #     parent = dataset['calibration_config']['sensors'][sensor_key]['parent_link']
    #     transform_key = parent + '-' + child
    #
    #     trans = list(dataset['collections'][selected_collection_key]['transforms'][transform_key]['trans'])
    #     quat = list(dataset['collections'][selected_collection_key]['transforms'][transform_key]['quat'])
    #     found = False
    #
    #     for joint in xml_robot.joints:
    #         if joint.parent == parent and joint.child == child:
    #             found = True
    #             print('Found joint: ' + str(joint.name))
    #
    #             print('Replacing xyz = ' + str(joint.origin.xyz) + ' by ' + str(trans))
    #             joint.origin.xyz = trans
    #
    #             rpy = list(tf.transformations.euler_from_quaternion(quat, axes='sxyz'))
    #             print('Replacing rpy = ' + str(joint.origin.rpy) + ' by ' + str(rpy))
    #             joint.origin.rpy = rpy
    #             break
    #
    #     if not found:
    #         raise ValueError('Could not find transform ' + str(transform_key) + ' in /robot_description')
    #
    # # TODO remove hardcoded xacro name
    # outfile = rospkg.RosPack().get_path('atom_calibration') + '/calibrations/atlascar2/optimized.urdf.xacro'
    # with open(outfile, 'w') as out:
    #     print("Writing optimized urdf to " + str(outfile))
    #     out.write(URDF.to_xml_string(xml_robot))
    #
    # print('Xacro file saved.')


if __name__ == "__main__":
    main()
